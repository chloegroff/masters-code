{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNET SEGMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "from glob import glob as glob\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.image as Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#tf.config.run_functions_eagerly(True)\n",
    "\n",
    "## Seeding \n",
    "seed = 2019\n",
    "random.seed = seed\n",
    "np.random.seed = seed\n",
    "tf.seed = seed\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 224\n",
    "\n",
    "# get image and mask files\n",
    "image_files = glob(r'C:\\Users\\chloe\\DE4\\Masters\\Dataset\\Training_Data_no8\\*_i*')\n",
    "mask_files = glob(r'C:\\Users\\chloe\\DE4\\Masters\\Dataset\\Training_Data_no8\\*_s*')\n",
    "\n",
    "# splits into test and training datasets\n",
    "train_imgs, test_imgs, train_masks, test_masks = train_test_split(image_files, mask_files, test_size = 0.33, random_state= 123)\n",
    "\n",
    "print(len(train_imgs))\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "count = 0\n",
    "ratios = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DataGen(keras.utils.Sequence):\n",
    "    def __init__(self, image_files, mask_files,  batch_size, hls = False):\n",
    "        self.img_files = image_files\n",
    "        self.mask_files = mask_files\n",
    "        self.batch_size = batch_size\n",
    "        self.num_files = len(image_files)\n",
    "\n",
    "        self.hls = hls\n",
    "\n",
    "        self.current_ratios = None\n",
    "\n",
    "    def ratio_Imgs(self, masks):\n",
    "        rs = []\n",
    "        for i in masks:\n",
    "            i = np.reshape(i, (dim, dim))\n",
    "\n",
    "            r = sum(i > 0.5)/ dim **2\n",
    "\n",
    "            rs.append(r)\n",
    "        \n",
    "        self.current_ratios = np.array(rs)\n",
    "        global ratios\n",
    "        ratios = self.current_ratios\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Returns batch of images and labels'\n",
    "\n",
    "        this_batch = self.batch_size\n",
    "        # Changes batch size for final batch (no data missed)\n",
    "        if(index+1)*self.batch_size > self.num_files:\n",
    "            this_batch = self.num_files - index*self.batch_size\n",
    "        \n",
    "        # sets limits for batch\n",
    "        start = index*self.batch_size\n",
    "        stop = start + this_batch\n",
    "\n",
    "        # takes images and masks files within batch\n",
    "        img_batch = self.img_files[start : stop]\n",
    "        mask_batch = self.mask_files[start : stop]\n",
    "\n",
    "        images = []\n",
    "        masks  = []\n",
    "        \n",
    "        # loads images and files and adds them to list\n",
    "        for i, (img_file, mask_file) in enumerate(zip(img_batch,mask_batch)):\n",
    "            # Normalises\n",
    "            img = cv2.imread(img_file) \n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            if self.hls:\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_RGB2HLS)\n",
    "            \n",
    "            img = img / 255\n",
    "\n",
    "            mask = cv2.imread(mask_file, 0) / 255\n",
    "            mask = np.array(mask, dtype=np.float32)\n",
    "            mask = np.reshape(mask, (mask.shape[0], mask.shape[1], 1))\n",
    "            images.append(img)\n",
    "            masks.append(mask)\n",
    "            \n",
    "        images = np.array(images)\n",
    "        #image = np.reshape(image, (batch_size, 256, 256, 1))\n",
    "        masks  = np.array(masks)\n",
    "        \n",
    "        self.ratio_Imgs(masks)\n",
    "\n",
    "        return images, masks\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        global count\n",
    "        count = 0\n",
    "    \n",
    "    def __len__(self):\n",
    "        'Returns Number of batches'\n",
    "        return int(np.ceil(self.num_files // self.batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader obj is created with training images and masks\n",
    "gen = DataGen(image_files=train_imgs, mask_files=train_masks, batch_size=batch_size)\n",
    "x, y = gen.__getitem__(0)\n",
    "print(x.shape, y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "r = random.randint(0, len(x)-1)\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax.imshow(x[r])\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "ax.imshow(np.reshape(y[r], (dim, dim)), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Convolutional Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def down_block(x, filters, kernel_size=(3, 3), padding=\"same\", strides=1):\n",
    "    c = keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(x)\n",
    "    c = keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(c)\n",
    "    p = keras.layers.MaxPool2D((2, 2), (2, 2))(c)\n",
    "    return c, p\n",
    "\n",
    "def up_block(x, skip, filters, kernel_size=(3, 3), padding=\"same\", strides=1):\n",
    "    us = keras.layers.UpSampling2D((2, 2))(x)\n",
    "    concat = keras.layers.Concatenate()([us, skip])\n",
    "    c = keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(concat)\n",
    "    c = keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(c)\n",
    "    return c\n",
    "\n",
    "def bottleneck(x, filters, kernel_size=(3, 3), padding=\"same\", strides=1):\n",
    "    c = keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(x)\n",
    "    c = keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(c)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UNet(image_size):\n",
    "    f = [16, 32, 64, 128, 256]\n",
    "    inputs = keras.layers.Input((image_size, image_size, 3))\n",
    "    \n",
    "    p0 = inputs\n",
    "    c1, p1 = down_block(p0, f[0]) #128 -> 64\n",
    "    c2, p2 = down_block(p1, f[1]) #64 -> 32\n",
    "    c3, p3 = down_block(p2, f[2]) #32 -> 16\n",
    "    c4, p4 = down_block(p3, f[3]) #16->8\n",
    "    \n",
    "    bn = bottleneck(p4, f[4])\n",
    "    \n",
    "    u1 = up_block(bn, c4, f[3]) #8 -> 16\n",
    "    u2 = up_block(u1, c3, f[2]) #16 -> 32\n",
    "    u3 = up_block(u2, c2, f[1]) #32 -> 64\n",
    "    u4 = up_block(u3, c1, f[0]) #64 -> 128\n",
    "    \n",
    "    outputs = keras.layers.Conv2D(1, (1, 1), padding=\"same\", activation=\"sigmoid\")(u4)\n",
    "    model = keras.models.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras import saving\n",
    "\n",
    "# @keras.saving.register_keras_serializable()\n",
    "def weighted_bincrossentropy(true, pred):\n",
    "    \"\"\"\n",
    "    Calculates weighted binary cross entropy. The weights are fixed.\n",
    "        \n",
    "    This can be useful for unbalanced catagories.\n",
    "    \n",
    "    Adjust the weights here depending on what is required.\n",
    "    \n",
    "    For example if there are 10x as many positive classes as negative classes,\n",
    "        if you adjust weight_zero = 1.0, weight_one = 0.1, then false positives \n",
    "        will be penalize 10 times as much as false negatives.\n",
    "    \"\"\"\n",
    "    # y = keras.backend.flatten(true)\n",
    "    # r = np.max(((keras.backend.sum(y).numpy()/dim**2)**3, 1e-8))\n",
    "    # calculate the binary cross entropy\n",
    "    bin_crossentropy = keras.backend.binary_crossentropy(true, pred)\n",
    "    \n",
    "    # apply the weights\n",
    "    weights = true * np.log(13) + (1. - true) * 0.5\n",
    "    weighted_bin_crossentropy = weights * bin_crossentropy \n",
    "\n",
    "    return keras.backend.mean(weighted_bin_crossentropy)\n",
    "\n",
    "# @keras.saving.register_keras_serializable()\n",
    "def mean_iou_loss(y_true, y_pred):\n",
    "\n",
    "    pred_sum = keras.backend.sum(y_pred)\n",
    "    gt_sum = keras.backend.sum(y_true)\n",
    "    intersection = keras.backend.sum(tf.math.multiply(y_true,y_pred))\n",
    "    \n",
    "    union = pred_sum + gt_sum - intersection\n",
    "\n",
    "    iou = intersection/union\n",
    "    return 1 - iou\n",
    "\n",
    "# @keras.saving.register_keras_serializable()\n",
    "def dice_coef(y_true, y_pred, smooth=1):\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    intersection = keras.backend.sum(y_true * y_pred, axis=[1,2,3])\n",
    "    union = keras.backend.sum(y_true, axis=[1,2,3]) + keras.backend.sum(y_pred, axis=[1,2,3])\n",
    "    return keras.backend.mean( (2. * intersection + smooth) / (union + smooth), axis=0)\n",
    "\n",
    "# @keras.saving.register_keras_serializable()\n",
    "def dice_loss(in_gt, in_pred):\n",
    "    return 1 - dice_coef(in_gt, in_pred)\n",
    "\n",
    "# @keras.saving.register_keras_serializable()\n",
    "def dice_p_bce(in_gt, in_pred):\n",
    "    \"\"\"combine DICE and BCE\"\"\"\n",
    "    # 0.01*keras.losses.binary_crossentropy(in_gt, in_pred)\n",
    "    return 0.01*weighted_bincrossentropy(in_gt, in_pred) + (1- dice_coef(in_gt, in_pred))\n",
    "\n",
    "# @keras.saving.register_keras_serializable()\n",
    "def true_positive_rate(y_true, y_pred):\n",
    "    return keras.backend.sum(keras.backend.flatten(y_true)*keras.backend.flatten(keras.backend.round(y_pred)))/keras.backend.sum(y_true)\n",
    "\n",
    "# @keras.saving.register_keras_serializable()\n",
    "def dice_mean_iou(in_gt, in_pred):\n",
    "    return (dice_loss(in_gt, in_pred) + mean_iou_loss(in_gt, in_pred)) / 2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet(dim)\n",
    "\n",
    "learning_rate = 0.0001\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "# opt = tf.keras.optimizers.SGD(learning_rate = 0.00001)\n",
    "# model.compile(optimizer = opt, loss=weighted_bincrossentropy, metrics=[\"acc\"])\n",
    "cce = keras.losses.CategoricalFocalCrossentropy()\n",
    "\n",
    "use_loss = dice_mean_iou\n",
    "print(use_loss.__name__)\n",
    "model.compile(optimizer = opt, loss=use_loss, metrics=[dice_coef, 'binary_accuracy', dice_p_bce, dice_loss, true_positive_rate, weighted_bincrossentropy])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = DataGen(train_imgs, train_masks, batch_size= batch_size, hls = False)\n",
    "valid_gen = DataGen(test_imgs, test_masks, batch_size=batch_size, hls = False)\n",
    "\n",
    "# from sklearn.utils import class_weight\n",
    "x, y = train_gen.__getitem__(5)\n",
    "\n",
    "w1s = []\n",
    "w2s = []\n",
    "for i in y:\n",
    "    i = np.reshape(i, (dim,dim))\n",
    "    if np.sum(i) != 0:\n",
    "        w1s.append(dim**2/(2 * np.sum(i)) )\n",
    "        w2s.append(dim**2/(2 * (dim**2 - sum(i))))\n",
    "\n",
    "print(np.mean(w1s), np.mean(w2s))\n",
    "\n",
    "\n",
    "rs = []\n",
    "for i in range(len(train_gen)):\n",
    "    _, masks = train_gen.__getitem__(i)\n",
    "\n",
    "    for j in masks:\n",
    "        j = np.reshape(j, (dim,dim))\n",
    "        r = sum(j > 0.5) / dim**2\n",
    "        rs.append(r)\n",
    "\n",
    "r = np.mean(rs)\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_num = 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "history = model.fit(train_gen, batch_size=batch_size, epochs = 350, validation_data= valid_gen) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['dice_coef'])\n",
    "plt.plot(history.history['val_dice_coef'])\n",
    "plt.title('model dice coefficent')\n",
    "plt.ylabel('dice coef')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Max acc:\", round(max(history.history['acc']),4))\n",
    "# print(\"Max val_acc:\", round(max(history.history['val_acc']),4))\n",
    "# print(\"\\nMin loss:\", round(min(history.history['loss']),4))\n",
    "# print(\"Min val_loss:\", round(min(history.history['val_loss']),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the Weights\n",
    "\n",
    "#model.save_weights(\"UNetW.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataset for prediction\n",
    "x, y = valid_gen.__getitem__(0)\n",
    "result = model.predict(x)\n",
    "\n",
    "result = result > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "\n",
    "r = random.randint(0, len(x)-1)\n",
    "\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax.imshow(np.reshape(y[r]*255, (dim, dim)), cmap=\"gray\")\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "ax.imshow(np.reshape(result[r]*255, (dim, dim)), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols = 3, )\n",
    "\n",
    "fig.set_figwidth(3.6)\n",
    "fig.set_figheight(2)\n",
    "\n",
    "\n",
    "axes[0].imshow(x[0])\n",
    "axes[0].set_xlabel('Raw Image')\n",
    "axes[1].imshow(np.reshape(y[0]*255, (dim, dim)), cmap=\"gray\")\n",
    "axes[1].set_xlabel('Ground Truth')\n",
    "axes[2].imshow(np.reshape(result[0]*255, (dim, dim)), cmap=\"gray\")\n",
    "axes[2].set_xlabel('Prediction')\n",
    "\n",
    "matplotlib.rcParams.update({'font.size': 12, \"font.family\": \"Times New Roman\"})\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    axes[i].tick_params(\n",
    "        axis='both',          # changes apply to the voth axis\n",
    "        which='both', \n",
    "        left = False,   # both major and minor ticks are affected\n",
    "        right = False,\n",
    "        bottom=False,      # ticks along the bottom edge are off\n",
    "        top=False,         # ticks along the top edge are off\n",
    "        labelbottom=False) # labels along the bottom edge are off\n",
    "    \n",
    "    axes[i].xaxis.set_ticklabels([])\n",
    "    axes[i].yaxis.set_ticklabels([])\n",
    "\n",
    "    for j in ax.spines:\n",
    "        axes[i].spines[j].set_visible(False)\n",
    "        \n",
    "    plt.savefig(rf'C:\\Users\\chloe\\DE4\\Masters\\Models\\Model_{model_num}_example.pdf', dpi =300)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir saved_models\n",
    "# model.save(r'C:\\Users\\thoma\\Desktop\\CT_Segment_Full_v2.h5')\n",
    "\n",
    "model.save(rf'C:\\Users\\chloe\\DE4\\Masters\\Models\\Model_{model_num}.keras')\n",
    "\n",
    "\n",
    "save_dict = {'History': history.history, 'Loss Function': use_loss.__name__, 'Learning Rate': learning_rate, 'Num Training Images': len(train_imgs), 'Num Test Images': len(test_imgs), 'Batch Size': batch_size}\n",
    "\n",
    "with open(rf'C:\\Users\\chloe\\DE4\\Masters\\Models\\Model_{model_num}_history.json', 'w') as f:\n",
    "    json.dump(save_dict, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(r'C:\\Users\\chloe\\DE4\\Masters\\Models\\Model_3_history.pkl', 'r') as f:\n",
    "#     hist = json.load(f)\n",
    "\n",
    "# print(len(hist['History']['binary_accuray']))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
